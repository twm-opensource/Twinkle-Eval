from twinkle_eval import TwinkleEvalRunner
import json


def run_taigi_evaluation():
    """åŸ·è¡Œå°èªè©¦é¡Œè©•æ¸¬"""

    # å»ºç«‹è©•æ¸¬åŸ·è¡Œå™¨
    runner = TwinkleEvalRunner("config.yaml")

    # åˆå§‹åŒ–
    print("ğŸ”§ åˆå§‹åŒ–è©•æ¸¬ç’°å¢ƒ...")
    runner.initialize()

    # åŸ·è¡Œè©•æ¸¬
    print("ğŸš€ é–‹å§‹è©•æ¸¬...")
    results = runner.run_evaluation(export_formats=["json", "csv", "html"])

    print(f"\nâœ… è©•æ¸¬å®Œæˆ!")
    print(f"ğŸ“ çµæœå·²å„²å­˜è‡³: {results}")

    return results


def analyze_results(result_file):
    """åˆ†æè©•æ¸¬çµæœ"""

    with open(result_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print("\n" + "=" * 50)
    print("ğŸ“Š è©•æ¸¬çµæœåˆ†æ")
    print("=" * 50)

    # ç¸½é«”æº–ç¢ºç‡
    print(f"\nç¸½é«”æº–ç¢ºç‡: {data['accuracy']:.2%}")

    # çµ±è¨ˆå„é¡Œç­”é¡Œç‹€æ³
    details = data['details']
    total_questions = len(details)
    correct_count = sum(1 for item in details if item['is_correct'])

    print(f"ç¸½é¡Œæ•¸: {total_questions}")
    print(f"ç­”å°é¡Œæ•¸: {correct_count}")
    print(f"ç­”éŒ¯é¡Œæ•¸: {total_questions - correct_count}")

    # éŒ¯èª¤åˆ†æ
    print("\nâŒ ç­”éŒ¯çš„é¡Œç›®:")
    wrong_questions = [item for item in details if not item['is_correct']]

    for item in wrong_questions[:5]:  # é¡¯ç¤ºå‰5é¡Œ
        print(f"\né¡Œè™Ÿ {item['question_id']}:")
        print(f"  æ­£ç¢ºç­”æ¡ˆ: {item['correct_answer']}")
        print(f"  æ¨¡å‹é æ¸¬: {item['predicted_answer']}")
        print(f"  é¡Œç›®: {item['question'][:100]}...")  # åªé¡¯ç¤ºå‰100å­—

    if len(wrong_questions) > 5:
        print(f"\n... é‚„æœ‰ {len(wrong_questions) - 5} é¡Œç­”éŒ¯")

    # é¸é …åˆ†å¸ƒåˆ†æ
    from collections import Counter
    predicted_answers = [item['predicted_answer'] for item in details]
    correct_answers = [item['correct_answer'] for item in details]

    print("\nğŸ“ˆ æ¨¡å‹é æ¸¬é¸é …åˆ†å¸ƒ:")
    pred_counter = Counter(predicted_answers)
    for option, count in sorted(pred_counter.items()):
        print(f"  {option}: {count} æ¬¡ ({count / total_questions:.1%})")

    print("\nğŸ“ˆ æ­£ç¢ºç­”æ¡ˆé¸é …åˆ†å¸ƒ:")
    correct_counter = Counter(correct_answers)
    for option, count in sorted(correct_counter.items()):
        print(f"  {option}: {count} æ¬¡ ({count / total_questions:.1%})")


if __name__ == "__main__":
    # åŸ·è¡Œè©•æ¸¬
    result_files = run_taigi_evaluation()

    # åˆ†æçµæœ (æ‰¾åˆ°è©³ç´°çµæœæª”æ¡ˆ)
    import glob

    eval_result_files = glob.glob("results/eval_results_*.json")
    if eval_result_files:
        latest_file = max(eval_result_files)  # å–æœ€æ–°çš„æª”æ¡ˆ
        analyze_results(latest_file)