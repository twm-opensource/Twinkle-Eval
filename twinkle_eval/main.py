import argparse
import copy
import os
from datetime import datetime
from typing import Any, Dict, List, Optional

import numpy as np

from twinkle_eval.exceptions import ConfigurationError

from .config import load_config
from .datasets import find_all_evaluation_files
from .evaluators import Evaluator
from .logger import log_error, log_info
from .results_exporters import ResultsExporterFactory


def create_default_config(output_path: str = "config.yaml") -> int:
    """å‰µå»ºé è¨­é…ç½®æª”æ¡ˆ

    Args:
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼Œé è¨­ç‚º config.yaml

    Returns:
        int: ç¨‹å¼é€€å‡ºä»£ç¢¼ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œ1 è¡¨ç¤ºå¤±æ•—ï¼‰
    """
    import shutil

    try:
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_path):
            response = input(f"âš ï¸  æª”æ¡ˆ '{output_path}' å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†è“‹ï¼Ÿ(y/N): ")
            if response.lower() not in ["y", "yes", "æ˜¯"]:
                print("âŒ å–æ¶ˆå‰µå»ºé…ç½®æª”æ¡ˆ")
                return 1

        # æ‰¾åˆ°ç¯„æœ¬æª”æ¡ˆ
        template_path = os.path.join(os.path.dirname(__file__), "config.template.yaml")

        if not os.path.exists(template_path):
            print(f"âŒ æ‰¾ä¸åˆ°é…ç½®ç¯„æœ¬æª”æ¡ˆ: {template_path}")
            return 1

        # è¤‡è£½ç¯„æœ¬æª”æ¡ˆ
        shutil.copy2(template_path, output_path)

        print(f"âœ… é…ç½®æª”æ¡ˆå·²å‰µå»º: {output_path}")
        print()
        print("ğŸ“ æ¥ä¸‹ä¾†è«‹ç·¨è¼¯é…ç½®æª”æ¡ˆï¼Œè¨­å®šï¼š")
        print("  1. LLM API è¨­å®š (base_url, api_key)")
        print("  2. æ¨¡å‹åç¨± (model.name)")
        print("  3. è³‡æ–™é›†è·¯å¾‘ (evaluation.dataset_paths)")
        print()
        print("ğŸ’¡ ç·¨è¼¯å®Œæˆå¾Œï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é–‹å§‹è©•æ¸¬ï¼š")
        print(f"   twinkle-eval --config {output_path}")

        return 0

    except Exception as e:
        print(f"âŒ å‰µå»ºé…ç½®æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return 1


class TwinkleEvalRunner:
    """Twinkle Eval ä¸»è¦åŸ·è¡Œå™¨é¡åˆ¥ - è² è²¬æ§åˆ¶æ•´å€‹è©•æ¸¬æµç¨‹"""

    def __init__(self, config_path: str = "config.yaml"):
        """åˆå§‹åŒ– Twinkle Eval åŸ·è¡Œå™¨

        Args:
            config_path: é…ç½®æª”æ¡ˆè·¯å¾‘ï¼Œé è¨­ç‚º config.yaml
        """
        self.config_path = config_path  # é…ç½®æª”æ¡ˆè·¯å¾‘
        self.config = None  # è¼‰å…¥çš„é…ç½®å­—å…¸
        self.start_time = None  # åŸ·è¡Œé–‹å§‹æ™‚é–“æ¨™è¨˜
        self.start_datetime = None  # åŸ·è¡Œé–‹å§‹çš„ datetime ç‰©ä»¶
        self.results_dir = "results"  # çµæœè¼¸å‡ºç›®éŒ„

    def initialize(self):
        """åˆå§‹åŒ–è©•æ¸¬åŸ·è¡Œå™¨

        è¼‰å…¥é…ç½®ã€è¨­å®šæ™‚é–“æ¨™è¨˜ã€å»ºç«‹çµæœç›®éŒ„

        Raises:
            Exception: åˆå§‹åŒ–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤
        """
        try:
            self.config = load_config(self.config_path)  # è¼‰å…¥é…ç½®
            self.start_time = datetime.now().strftime("%Y%m%d_%H%M")  # ç”Ÿæˆæ™‚é–“æ¨™è¨˜
            self.start_datetime = datetime.now()  # è¨˜éŒ„é–‹å§‹æ™‚é–“

            os.makedirs(self.results_dir, exist_ok=True)  # å»ºç«‹çµæœç›®éŒ„

            log_info(f"Twinkle Eval åˆå§‹åŒ–å®Œæˆ - {self.start_time}")

        except Exception as e:
            log_error(f"åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def _prepare_config_for_saving(self) -> Dict[str, Any]:
        """æº–å‚™ç”¨æ–¼å„²å­˜çš„é…ç½®è³‡æ–™ï¼Œç§»é™¤æ•æ„Ÿè³‡è¨Š

        åœ¨å„²å­˜é…ç½®åˆ°çµæœæª”æ¡ˆå‰ï¼Œéœ€è¦ç§»é™¤ API é‡‘é‘°ç­‰æ•æ„Ÿè³‡è¨Š
        å’Œä¸å¯åºåˆ—åŒ–çš„ç‰©ä»¶å¯¦ä¾‹

        Returns:
            Dict[str, Any]: æ¸…ç†å¾Œçš„é…ç½®å­—å…¸
        """
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        save_config = copy.deepcopy(self.config)

        # ç§»é™¤æ•æ„Ÿè³‡è¨Šï¼ˆAPI é‡‘é‘°ï¼‰
        if "llm_api" in save_config and "api_key" in save_config["llm_api"]:
            del save_config["llm_api"]["api_key"]

        # ç§»é™¤ç‰©ä»¶å¯¦ä¾‹ï¼ˆä¸å¯åºåˆ—åŒ–ï¼‰
        if "llm_instance" in save_config:
            del save_config["llm_instance"]
        if "evaluation_strategy_instance" in save_config:
            del save_config["evaluation_strategy_instance"]

        return save_config

    def _get_dataset_paths(self) -> List[str]:
        """å¾é…ç½®ä¸­å–å¾—è³‡æ–™é›†è·¯å¾‘æ¸…å–®

        æ”¯æ´å–®ä¸€è·¯å¾‘å­—ä¸²æˆ–è·¯å¾‘æ¸…å–®ï¼Œçµ±ä¸€è½‰æ›ç‚ºæ¸…å–®æ ¼å¼

        Returns:
            List[str]: è³‡æ–™é›†è·¯å¾‘æ¸…å–®
        """
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        dataset_paths = self.config["evaluation"]["dataset_paths"]
        if isinstance(dataset_paths, str):
            dataset_paths = [dataset_paths]
        return dataset_paths

    def _evaluate_dataset(self, dataset_path: str, evaluator: Evaluator) -> Dict[str, Any]:
        """è©•æ¸¬å–®ä¸€è³‡æ–™é›†

        å°æŒ‡å®šè³‡æ–™é›†ä¸­çš„æ‰€æœ‰æª”æ¡ˆé€²è¡Œè©•æ¸¬ï¼Œæ”¯æ´å¤šæ¬¡åŸ·è¡Œä¸¦çµ±è¨ˆçµæœ

        Args:
            dataset_path: è³‡æ–™é›†è·¯å¾‘
            evaluator: è©•æ¸¬å™¨å¯¦ä¾‹

        Returns:
            Dict[str, Any]: è³‡æ–™é›†è©•æ¸¬çµæœï¼ŒåŒ…å«æº–ç¢ºç‡çµ±è¨ˆå’Œè©³ç´°çµæœ
        """
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        log_info(f"é–‹å§‹è©•æ¸¬è³‡æ–™é›†: {dataset_path}")

        all_files = find_all_evaluation_files(dataset_path)  # å°‹æ‰¾æ‰€æœ‰è©•æ¸¬æª”æ¡ˆ
        repeat_runs = self.config["evaluation"].get("repeat_runs", 1)  # é‡è¤‡åŸ·è¡Œæ¬¡æ•¸
        prompt_map = self.config["evaluation"].get("datasets_prompt_map", {})  # è³‡æ–™é›†èªè¨€å°æ‡‰è¡¨
        dataset_lang = prompt_map.get(dataset_path, "zh")  # ç•¶å‰è³‡æ–™é›†çš„èªè¨€ï¼Œé è¨­ç‚ºä¸­æ–‡

        results = []  # å„²å­˜æ‰€æœ‰æª”æ¡ˆçš„è©•æ¸¬çµæœ

        for idx, file_path in enumerate(all_files):
            file_accuracies = []  # ç•¶å‰æª”æ¡ˆçš„æº–ç¢ºç‡çµæœ
            file_results = []  # ç•¶å‰æª”æ¡ˆçš„è©³ç´°çµæœ

            # å°ç•¶å‰æª”æ¡ˆé€²è¡Œå¤šæ¬¡è©•æ¸¬
            for run in range(repeat_runs):
                try:
                    file_path_result, accuracy, result_path = evaluator.evaluate_file(
                        file_path, f"{self.start_time}_run{run}", dataset_lang
                    )
                    file_accuracies.append(accuracy)
                    file_results.append((file_path_result, accuracy, result_path))
                except Exception as e:
                    log_error(f"è©•æ¸¬æª”æ¡ˆ {file_path} å¤±æ•—: {e}")
                    continue

            # ç‚ºç•¶å‰æª”æ¡ˆè¨ˆç®—çµ±è¨ˆæ•¸æ“š
            if file_accuracies:
                mean_accuracy = np.mean(file_accuracies)  # å¹³å‡æº–ç¢ºç‡
                std_accuracy = np.std(file_accuracies) if len(file_accuracies) > 1 else 0  # æ¨™æº–å·®

                results.append(
                    {
                        "file": file_path,
                        "accuracy_mean": mean_accuracy,
                        "accuracy_std": std_accuracy,
                        "individual_runs": {
                            "accuracies": file_accuracies,
                            "results": [r[2] for r in file_results],
                        },
                    }
                )

            # é€²åº¦æŒ‡ç¤ºå™¨
            progress = (idx + 1) / len(all_files) * 100
            print(f"\rå·²åŸ·è¡Œ {progress:.1f}% ({idx + 1}/{len(all_files)}) ", end="")

        print()  # é€²åº¦å®Œæˆå¾Œæ›è¡Œ

        # è¨ˆç®—è³‡æ–™é›†çµ±è¨ˆæ•¸æ“š
        dataset_avg_accuracy = (
            np.mean([r["accuracy_mean"] for r in results]) if results else 0
        )  # è³‡æ–™é›†å¹³å‡æº–ç¢ºç‡
        dataset_avg_std = (
            np.mean([r["accuracy_std"] for r in results]) if results else 0
        )  # è³‡æ–™é›†å¹³å‡æ¨™æº–å·®

        return {
            "results": results,
            "average_accuracy": dataset_avg_accuracy,
            "average_std": dataset_avg_std,
        }

    def run_evaluation(self, export_formats: Optional[List[str]] = None) -> str:
        """åŸ·è¡Œå®Œæ•´çš„è©•æ¸¬æµç¨‹

        é€™æ˜¯ä¸»è¦çš„è©•æ¸¬å…¥å£é»ï¼ŒåŒ…å«ä»¥ä¸‹æ­¥é©Ÿï¼š
        1. å»ºç«‹è©•æ¸¬å™¨
        2. å°æ‰€æœ‰è³‡æ–™é›†é€²è¡Œè©•æ¸¬
        3. çµ±è¨ˆå’Œè¼¸å‡ºçµæœ

        Args:
            export_formats: è¼¸å‡ºæ ¼å¼æ¸…å–®ï¼Œé è¨­ç‚º ["json"]

        Returns:
            str: ä¸»è¦çµæœæª”æ¡ˆè·¯å¾‘
        """
        if self.config is None:
            raise ConfigurationError("é…ç½®æœªè¼‰å…¥")

        if export_formats is None:
            export_formats = ["json"]  # é è¨­è¼¸å‡ºæ ¼å¼

        dataset_paths = self._get_dataset_paths()  # å–å¾—è³‡æ–™é›†è·¯å¾‘
        dataset_results = {}  # å„²å­˜æ‰€æœ‰è³‡æ–™é›†çš„çµæœ

        # å»ºç«‹è©•æ¸¬å™¨
        llm_instance = self.config["llm_instance"]
        evaluation_strategy_instance = self.config["evaluation_strategy_instance"]
        evaluator = Evaluator(llm_instance, evaluation_strategy_instance, self.config)

        # é€ä¸€è©•æ¸¬æ¯å€‹è³‡æ–™é›†
        for dataset_path in dataset_paths:
            try:
                dataset_result = self._evaluate_dataset(dataset_path, evaluator)
                dataset_results[dataset_path] = dataset_result

                message = (
                    f"è³‡æ–™é›† {dataset_path} è©•æ¸¬å®Œæˆï¼Œ"
                    f"å¹³å‡æ­£ç¢ºç‡: {dataset_result['average_accuracy']:.2%} "
                    f"(Â±{dataset_result['average_std']:.2%})"
                )
                print(message)
                log_info(message)

            except Exception as e:
                log_error(f"è³‡æ–™é›† {dataset_path} è©•æ¸¬å¤±æ•—: {e}")
                continue

        # æº–å‚™æœ€çµ‚çµæœ
        current_duration = (
            (datetime.now() - self.start_datetime).total_seconds() if self.start_datetime else 0
        )  # è¨ˆç®—åŸ·è¡Œæ™‚é–“
        final_results = {
            "timestamp": self.start_time,  # åŸ·è¡Œæ™‚é–“æ¨™è¨˜
            "config": self._prepare_config_for_saving(),  # æ¸…ç†å¾Œçš„é…ç½®
            "dataset_results": dataset_results,  # æ‰€æœ‰è³‡æ–™é›†çµæœ
            "duration_seconds": current_duration,  # åŸ·è¡Œæ™‚é–“ï¼ˆç§’ï¼‰
        }

        # ä»¥å¤šç¨®æ ¼å¼è¼¸å‡ºçµæœ
        base_output_path = os.path.join(self.results_dir, f"results_{self.start_time}")
        exported_files = ResultsExporterFactory.export_results(
            final_results, base_output_path, export_formats
        )

        log_info(f"è©•æ¸¬å®Œæˆï¼Œçµæœå·²åŒ¯å‡ºè‡³: {', '.join(exported_files)}")
        return exported_files[0] if exported_files else ""


def create_cli_parser() -> argparse.ArgumentParser:
    """å»ºç«‹å‘½ä»¤åˆ—ä»‹é¢è§£æå™¨

    å®šç¾©æ‰€æœ‰å‘½ä»¤åˆ—åƒæ•¸å’Œé¸é …ï¼Œæ”¯æ´å¤šç¨®è©•æ¸¬å’ŒæŸ¥è©¢åŠŸèƒ½

    Returns:
        argparse.ArgumentParser: é…ç½®å®Œæˆçš„å‘½ä»¤åˆ—è§£æå™¨
    """
    parser = argparse.ArgumentParser(
        description="ğŸŒŸ Twinkle Eval - AI æ¨¡å‹è©•æ¸¬å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  twinkle-eval                          # ä½¿ç”¨é è¨­é…ç½®åŸ·è¡Œ
  twinkle-eval --config custom.yaml    # ä½¿ç”¨è‡ªå®šç¾©é…ç½®æª”
  twinkle-eval --export json csv html  # è¼¸å‡ºç‚ºå¤šç¨®æ ¼å¼
  twinkle-eval --list-llms             # åˆ—å‡ºå¯ç”¨çš„ LLM é¡å‹
  twinkle-eval --list-strategies       # åˆ—å‡ºå¯ç”¨çš„è©•æ¸¬ç­–ç•¥
        """,
    )

    parser.add_argument(
        "--config", "-c", default="config.yaml", help="é…ç½®æª”æ¡ˆè·¯å¾‘ (é è¨­: config.yaml)"
    )

    parser.add_argument(
        "--export",
        "-e",
        nargs="+",
        default=["json"],
        choices=ResultsExporterFactory.get_available_types(),
        help="è¼¸å‡ºæ ¼å¼ (é è¨­: json)",
    )

    parser.add_argument("--list-llms", action="store_true", help="åˆ—å‡ºå¯ç”¨çš„ LLM é¡å‹")

    parser.add_argument("--list-strategies", action="store_true", help="åˆ—å‡ºå¯ç”¨çš„è©•æ¸¬ç­–ç•¥")

    parser.add_argument("--list-exporters", action="store_true", help="åˆ—å‡ºå¯ç”¨çš„è¼¸å‡ºæ ¼å¼")

    parser.add_argument("--version", action="store_true", help="é¡¯ç¤ºç‰ˆæœ¬è³‡è¨Š")

    parser.add_argument("--init", action="store_true", help="å‰µå»ºé è¨­é…ç½®æª”æ¡ˆ")

    return parser


def main() -> int:
    """ä¸»ç¨‹å¼å…¥å£é»

    è™•ç†å‘½ä»¤åˆ—åƒæ•¸ä¸¦åŸ·è¡Œç›¸æ‡‰çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬æŸ¥è©¢åŠŸèƒ½å’Œä¸»è¦è©•æ¸¬æµç¨‹

    Returns:
        int: ç¨‹å¼é€€å‡ºä»£ç¢¼ï¼ˆ0 è¡¨ç¤ºæˆåŠŸï¼Œ1 è¡¨ç¤ºå¤±æ•—ï¼‰
    """
    parser = create_cli_parser()
    args = parser.parse_args()

    # è™•ç†æŸ¥è©¢å‘½ä»¤
    if args.list_llms:
        from .models import LLMFactory

        print("å¯ç”¨çš„ LLM é¡å‹:")
        for llm_type in LLMFactory.get_available_types():
            print(f"  - {llm_type}")
        return 0

    if args.list_strategies:
        from .evaluation_strategies import EvaluationStrategyFactory

        print("å¯ç”¨çš„è©•æ¸¬ç­–ç•¥:")
        for strategy in EvaluationStrategyFactory.get_available_types():
            print(f"  - {strategy}")
        return 0

    if args.list_exporters:
        print("å¯ç”¨çš„è¼¸å‡ºæ ¼å¼:")
        for exporter in ResultsExporterFactory.get_available_types():
            print(f"  - {exporter}")
        return 0

    if args.version:
        from . import get_info

        info = get_info()
        print(f"ğŸŒŸ {info['name']} v{info['version']}")
        print(f"ä½œè€…: {info['author']}")
        print(f"æˆæ¬Š: {info['license']}")
        print(f"ç¶²å€: {info['url']}")
        return 0

    if args.init:
        return create_default_config()

    # åŸ·è¡Œè©•æ¸¬
    try:
        runner = TwinkleEvalRunner(args.config)
        runner.initialize()
        runner.run_evaluation(args.export)
    except Exception as e:
        log_error(f"åŸ·è¡Œå¤±æ•—: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
